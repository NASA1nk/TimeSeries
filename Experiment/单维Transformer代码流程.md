# 数据处理

## 归一化

对数据做归一化处理，提升模型的收敛速度，提升模型的精度

> 多维数据，范围不一样，更需要最大值最小值归一化到一个相同的范围中

```python
scaler = MinMaxScaler(feature_range=(-1, 1))
# reshape()更改数据的行列数，(-1, 1)将series变为一个二维list(2203,1)（一列），归一化后再(-1)变为一个list(2203,)（一行）
amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)

# 反归一化：reamplitude = scaler.inverse_transform(amplitude.reshape(-1, 1)).reshape(-1)
```



## 获取训练集

**CSV文件value是一列数据**，处理CSV得到模型的训练集

- 每次从数据集中取出目标窗口长度的数据（input_window = 100）
  - 因为**预测步长是1**（output_window = 1），所以数据集的(i,i+100)部分是输入，数据集的(i+1,i+100+1)部分是**输出（label）**
    - 即训练集是一个二维list：[([0-100],[1-101]),([1-101],[2-102])...]
  - 可以看到，**下一个输入的seq就是上一个输入的label**

```python
train_sequence = create_inout_sequences(train_data, input_window)

def create_inout_sequences(input_data, tw):
    inout_seq = []
    L = len(input_data)
    for i in range(L-tw):
        train_seq = input_data[i:i+tw]
        train_label = input_data[i+output_window:i+tw+output_window]
        inout_seq.append((train_seq, train_label))
    return torch.FloatTensor(inout_seq)
```

> 要将训练集转换为torch.Tensor
>
> 每次预测其实生成了一个窗口大小的数据，即100个数据，因为0-99都是知道的，所以只看最后，是生成了100-99=1个新的数据值

```json
// train_seq
// [0-100],[1-101]...
array([-0.57387128, -0.40206378, -0.55946735, -0.60425452, -0.44742511,
       -0.58419515, -0.61932266, -0.69295419, -0.58351166, -0.54994044,
       -0.43662703, -0.61914435, -0.85068306, -0.48820822, -0.49690824,
       -0.60852687, -0.30539302, -0.67166959, -0.81256915, -0.72588739,
       -0.63647265, -0.35187454, -0.48203332, -0.46631505, -0.20902319,
       -0.55609022, -0.60737069, -0.63376441, -0.62251862, -0.83177698,
       -0.69605854, -0.59591266, -0.64712982, -0.72117908, -0.56725734,
       -0.49523732, -0.28346511, -0.40331179, -0.22654798, -0.49355597,
       -0.40401442, -0.55644669, -0.51327605, -0.54594082, -0.67255346,
       -0.5768457 , -0.68445274, -0.68570625, -0.50959873, -0.53279995,
       -0.69369626, -0.77043888, -0.47404523, -0.53040862, -0.55402487,
       -0.52105681, -0.6572804 , -0.32012793, -0.45812272, -0.53512967,
       -0.44819954, -0.62960976, -0.5902882 , -0.54241652, -0.5205725 ,
       -0.52635618, -0.70176632, -0.60213327, -0.61924325, -0.43668441,
       -0.45742335, -0.41136202, -0.39301151, -0.80772151, -0.6156968 ,
       -0.53395663, -0.60273659, -0.36046779, -0.50490417, -0.44372466,
       -0.4931788 , -0.4855132 , -0.41544793, -0.4899432 , -0.34648403,
       -0.58506004, -0.59291681, -0.53426938, -0.41909785, -0.4734859 ,
       -0.49628204, -0.42389299, -0.5318    , -0.62608821, -0.4355949 ,
       -0.44864787, -0.35575533, -0.52354333, -0.55667681, -0.6139897 ])

// train_label
// [1,101],[2,102]...
array([-0.40206378, -0.55946735, -0.60425452, -0.44742511, -0.58419515,
       -0.61932266, -0.69295419, -0.58351166, -0.54994044, -0.43662703,
       -0.61914435, -0.85068306, -0.48820822, -0.49690824, -0.60852687,
       -0.30539302, -0.67166959, -0.81256915, -0.72588739, -0.63647265,
       -0.35187454, -0.48203332, -0.46631505, -0.20902319, -0.55609022,
       -0.60737069, -0.63376441, -0.62251862, -0.83177698, -0.69605854,
       -0.59591266, -0.64712982, -0.72117908, -0.56725734, -0.49523732,
       -0.28346511, -0.40331179, -0.22654798, -0.49355597, -0.40401442,
       -0.55644669, -0.51327605, -0.54594082, -0.67255346, -0.5768457 ,
       -0.68445274, -0.68570625, -0.50959873, -0.53279995, -0.69369626,
       -0.77043888, -0.47404523, -0.53040862, -0.55402487, -0.52105681,
       -0.6572804 , -0.32012793, -0.45812272, -0.53512967, -0.44819954,
       -0.62960976, -0.5902882 , -0.54241652, -0.5205725 , -0.52635618,
       -0.70176632, -0.60213327, -0.61924325, -0.43668441, -0.45742335,
       -0.41136202, -0.39301151, -0.80772151, -0.6156968 , -0.53395663,
       -0.60273659, -0.36046779, -0.50490417, -0.44372466, -0.4931788 ,
       -0.4855132 , -0.41544793, -0.4899432 , -0.34648403, -0.58506004,
       -0.59291681, -0.53426938, -0.41909785, -0.4734859 , -0.49628204,
       -0.42389299, -0.5318    , -0.62608821, -0.4355949 , -0.44864787,
       -0.35575533, -0.52354333, -0.55667681, -0.6139897 , -0.50292192])

// 训练集的一个输入和输出
[(array([-0.57387128, -0.40206378, -0.55946735, -0.60425452, -0.44742511,
       -0.58419515, -0.61932266, -0.69295419, -0.58351166, -0.54994044,
       -0.43662703, -0.61914435, -0.85068306, -0.48820822, -0.49690824,
       -0.60852687, -0.30539302, -0.67166959, -0.81256915, -0.72588739,
       -0.63647265, -0.35187454, -0.48203332, -0.46631505, -0.20902319,
       -0.55609022, -0.60737069, -0.63376441, -0.62251862, -0.83177698,
       -0.69605854, -0.59591266, -0.64712982, -0.72117908, -0.56725734,
       -0.49523732, -0.28346511, -0.40331179, -0.22654798, -0.49355597,
       -0.40401442, -0.55644669, -0.51327605, -0.54594082, -0.67255346,
       -0.5768457 , -0.68445274, -0.68570625, -0.50959873, -0.53279995,
       -0.69369626, -0.77043888, -0.47404523, -0.53040862, -0.55402487,
       -0.52105681, -0.6572804 , -0.32012793, -0.45812272, -0.53512967,
       -0.44819954, -0.62960976, -0.5902882 , -0.54241652, -0.5205725 ,
       -0.52635618, -0.70176632, -0.60213327, -0.61924325, -0.43668441,
       -0.45742335, -0.41136202, -0.39301151, -0.80772151, -0.6156968 ,
       -0.53395663, -0.60273659, -0.36046779, -0.50490417, -0.44372466,
       -0.4931788 , -0.4855132 , -0.41544793, -0.4899432 , -0.34648403,
       -0.58506004, -0.59291681, -0.53426938, -0.41909785, -0.4734859 ,
       -0.49628204, -0.42389299, -0.5318    , -0.62608821, -0.4355949 ,
       -0.44864787, -0.35575533, -0.52354333, -0.55667681, -0.6139897 ]), array([-0.40206378, -0.55946735, -0.60425452, -0.44742511, -0.58419515,
       -0.61932266, -0.69295419, -0.58351166, -0.54994044, -0.43662703,
       -0.61914435, -0.85068306, -0.48820822, -0.49690824, -0.60852687,
       -0.30539302, -0.67166959, -0.81256915, -0.72588739, -0.63647265,
       -0.35187454, -0.48203332, -0.46631505, -0.20902319, -0.55609022,
       -0.60737069, -0.63376441, -0.62251862, -0.83177698, -0.69605854,
       -0.59591266, -0.64712982, -0.72117908, -0.56725734, -0.49523732,
       -0.28346511, -0.40331179, -0.22654798, -0.49355597, -0.40401442,
       -0.55644669, -0.51327605, -0.54594082, -0.67255346, -0.5768457 ,
       -0.68445274, -0.68570625, -0.50959873, -0.53279995, -0.69369626,
       -0.77043888, -0.47404523, -0.53040862, -0.55402487, -0.52105681,
       -0.6572804 , -0.32012793, -0.45812272, -0.53512967, -0.44819954,
       -0.62960976, -0.5902882 , -0.54241652, -0.5205725 , -0.52635618,
       -0.70176632, -0.60213327, -0.61924325, -0.43668441, -0.45742335,
       -0.41136202, -0.39301151, -0.80772151, -0.6156968 , -0.53395663,
       -0.60273659, -0.36046779, -0.50490417, -0.44372466, -0.4931788 ,
       -0.4855132 , -0.41544793, -0.4899432 , -0.34648403, -0.58506004,
       -0.59291681, -0.53426938, -0.41909785, -0.4734859 , -0.49628204,
       -0.42389299, -0.5318    , -0.62608821, -0.4355949 , -0.44864787,
       -0.35575533, -0.52354333, -0.55667681, -0.6139897 , -0.50292192]))]
```

**所以2000长度的文件，总共可以生成2000 - 100 = 1900个数据，每个数据中包含两个窗口的数据，各长100，即(1900,2,100)**

然后减去预测窗口（output_target = 1），即**1900 - 1 = 1899**个数据**(1899,2,100)**

```python
train_sequence = train_sequence[:-output_window]
```

```json
tensor([
    	[[-0.5739, -0.4021, -0.5595,  ..., -0.5235, -0.5567, -0.6140],
         [-0.4021, -0.5595, -0.6043,  ..., -0.5567, -0.6140, -0.5029]],

        [[-0.4021, -0.5595, -0.6043,  ..., -0.5567, -0.6140, -0.5029],
         [-0.5595, -0.6043, -0.4474,  ..., -0.6140, -0.5029, -0.6021]],

        [[-0.5595, -0.6043, -0.4474,  ..., -0.6140, -0.5029, -0.6021],
         [-0.6043, -0.4474, -0.5842,  ..., -0.5029, -0.6021, -0.5862]],

        ...,

        [[-0.6140, -0.5029, -0.6021,  ..., -0.4409, -0.4944, -0.6812],
         [-0.5029, -0.6021, -0.5862,  ..., -0.4944, -0.6812, -0.5717]],

        [[-0.5029, -0.6021, -0.5862,  ..., -0.4944, -0.6812, -0.5717],
         [-0.6021, -0.5862, -0.5648,  ..., -0.6812, -0.5717, -0.8837]],

        [[-0.6021, -0.5862, -0.5648,  ..., -0.6812, -0.5717, -0.8837],
         [-0.5862, -0.5648, -0.2523,  ..., -0.5717, -0.8837, -0.6390]]
])
```

数据长度为2203，2000是训练集，2203 - 2000 = 203是测试集

- 同理，最后得到**203 - 100 - 1 = 102**个数据**(102,2,100)**

# 模型

## 位置编码

> 维度就是列

pe是5000个`d_model = 250`维的tensor，即**(5000,250)**

```json
tensor([
    	[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,
          0.0000e+00,  1.0000e+00],
        [ 8.4147e-01,  5.4030e-01,  8.0100e-01,  ...,  1.0000e+00,
          1.0765e-04,  1.0000e+00],
        [ 9.0930e-01, -4.1615e-01,  9.5906e-01,  ...,  1.0000e+00,
          2.1529e-04,  1.0000e+00],
        ...,
        [ 9.5625e-01, -2.9254e-01, -9.4216e-01,  ...,  8.3699e-01,
          5.1234e-01,  8.5878e-01],
        [ 2.7050e-01, -9.6272e-01, -2.9535e-01,  ...,  8.3692e-01,
          5.1243e-01,  8.5873e-01],
        [-6.6395e-01, -7.4778e-01,  5.8825e-01,  ...,  8.3686e-01,
          5.1253e-01,  8.5867e-01]
       ])
```

position则是5000个1维的tensor，即**(5000,1)**，用于给pe的250个维度填充值（依次填充第1列，第2列...第250列）

```json
// 值从1到4999
tensor([[0.0000e+00],
        [1.0000e+00],
        [2.0000e+00],
        ...,
        [4.9970e+03],
        [4.9980e+03],
        [4.9990e+03]])
```

填充pe后再改变pe的形状，升维后转置变成**(5000,1,250)**

- **即5000个数据，每个数据是一个窗口，窗口内有一个长度是d_model = 250维的窗口**

> pe的维度一定是为了适配输入x的形状

```json
tensor([
    	[
            [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,
           0.0000e+00,  1.0000e+00]
        ],

        [
            [ 8.4147e-01,  5.4030e-01,  8.0100e-01,  ...,  1.0000e+00,
           1.0765e-04,  1.0000e+00]
        ],

        [
            [ 9.0930e-01, -4.1615e-01,  9.5906e-01,  ...,  1.0000e+00,
           2.1529e-04,  1.0000e+00]
        ],

        ...,

        [
    		[ 9.5625e-01, -2.9254e-01, -9.4216e-01,  ...,  8.3699e-01,
           5.1234e-01,  8.5878e-01]
        ],

        [
       		[ 2.7050e-01, -9.6272e-01, -2.9535e-01,  ...,  8.3692e-01,
           5.1243e-01,  8.5873e-01]
		],

        [
            [-6.6395e-01, -7.4778e-01,  5.8825e-01,  ...,  8.3686e-01,
           5.1253e-01,  8.5867e-01]
        ]
])
```

## Encoder

[Transformer — PyTorch 1.11.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html?highlight=transformer#torch.nn.Transformer)

**batch_first**

- If `True`, then the input and output tensors are provided as **(batch, seq, feature)**. 
- Default: `False` (seq, batch, feature).

```python
self.transformer_encoder = nn.TransformerEncoder(
            self.encoder_layer, num_layers=num_layers)
```

> 所以后面(20,100,1)要转换为(100,20,1)

## Liner

`torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)`

- **in_features** – size of each input sample
- **out_features** – size of each output sample
- **bias** – If set to `False`, the layer will not learn an additive bias. Default: `True`

```python
self.decoder = nn.Linear(feature_size, 1)
```

# 训练

## Epoch

epochs = 100，**从第一个epoch到第100个epoch，每个epoch都是拿一整个完整的数据集进行训练**

- **一个Epoch的训练次数 = 数据集长度 / batch_size**

> 进入model.trainI()函数

```python
# 标准写法
# range(0,len(train_data),batch_size)，并不是enumerate的参数
# batch是数据集索引，表示当前是第几个batch数据
# i是数据集开头的索引，表示当前数据集从整个数据的序列i开始
for batch, i in enumerate( range(0, len(train_data) - 1, batch_size) ):
    data, targets = get_batch(train_data, i, batch_size)
```

## Batch

通过i来在训练集上截取每次batch的数据

**batch_size = 20，每次从训练集中取20个数据，即(20,2,100)**，最后一个不足20则取剩余所有的数据

- (batch,i)：(0,0),(1,20),(2,40)...

> batch_size是第一维的参数

```python
data = source[i:i+seq_len]
```

```json
tensor([
    	[[-0.5161, -0.5684, -0.5169,  ..., -0.5884, -0.7686, -0.4507],
         [-0.5684, -0.5169, -0.6286,  ..., -0.7686, -0.4507, -0.5368]],

        [[-0.5684, -0.5169, -0.6286,  ..., -0.7686, -0.4507, -0.5368],
         [-0.5169, -0.6286, -0.3985,  ..., -0.4507, -0.5368, -0.4574]],

        [[-0.5169, -0.6286, -0.3985,  ..., -0.4507, -0.5368, -0.4574],
         [-0.6286, -0.3985, -0.5671,  ..., -0.5368, -0.4574, -0.6373]],

        ...,

        [[-0.8046, -0.4976, -0.8442,  ..., -0.7260, -0.6980, -0.5960],
         [-0.4976, -0.8442, -0.6934,  ..., -0.6980, -0.5960, -0.5318]],

        [[-0.4976, -0.8442, -0.6934,  ..., -0.6980, -0.5960, -0.5318],
         [-0.8442, -0.6934, -0.8517,  ..., -0.5960, -0.5318, -0.4263]],

        [[-0.8442, -0.6934, -0.8517,  ..., -0.5960, -0.5318, -0.4263],
         [-0.6934, -0.8517, -0.7266,  ..., -0.5318, -0.4263, -0.6205]]
		],
       device='cuda:0')
```

## 训练数据

从本次训练集的batch部分数据**(20,2,100)**中获取本次的输入

- 20个数据中，每个数据的**第一个列表数据item[0]是输入[0-100],[1,101]...**
- 20个数据中，每个数据的**第二个列表数据item[1]是输出[1-101],[2,102]**

```python
input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window, 1))
target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window, 1))
```
要将所有的**输入item[0]**拼接在一起，所有的输出**item[1]**拼接在一起

- t = [item[0] for item in data]，即本次batch数据中所有的输入（20个长度为100的**tensor list**）
- torch.stack(inputs, dim=?)，对inputs（多个tensor）沿指定维度dim拼接，返回一维的tensor
  - **即这20个输入拼接成一个(20,100)的tensor，一共batch_size = 20行，input_window = 100列**
- torch.chunk(tensor, chunk_num, dim)，将tensor在指定维度上（0行，1列）分为chunk_num块，返回一个tensor tuple
  - torch.chunk(x,100,1)，因为**有input_window = 100列，所以在列上将输入(20,100)分成100块，每一块有batch_size = 20个数据**，得到**100个(20,1)的tuple**
- 最后拼成一列，得到**(100,20,1)**，这就是要输入到网络中的数据

> 一共batch_size = 20列，input_window = 100行，这个就是处理的逻辑torch.chunk
>
> 一个数据就是一个list了，[value]

```json
// 最开始获取的所有的输入，(20,100)
tensor([
            [-0.5161, -0.5684, -0.5169,  ..., -0.5884, -0.7686, -0.4507],
            [-0.5684, -0.5169, -0.6286,  ..., -0.7686, -0.4507, -0.5368],
            [-0.5169, -0.6286, -0.3985,  ..., -0.4507, -0.5368, -0.4574],
            ...,
            [-0.8046, -0.4976, -0.8442,  ..., -0.7260, -0.6980, -0.5960],
            [-0.4976, -0.8442, -0.6934,  ..., -0.6980, -0.5960, -0.5318],
            [-0.8442, -0.6934, -0.8517,  ..., -0.5960, -0.5318, -0.4263]
       ],
       device='cuda:0')

// 将(20,100)切分为100长度的tuple，每个是一个(20,1)的tensor，即x的20行的一列数据，一共100个，即(100,20,1)
(
tensor([[-0.5161],
        [-0.5684],
        [-0.5169],
        [-0.6286],
        [-0.3985],
        [-0.5671],
        [-0.4416],
        [-0.7152],
        [-0.6121],
        [-0.7587],
        [-0.5539],
        [-0.4342],
        [-0.2928],
        [-0.2900],
        [-0.6720],
        [-0.7235],
        [-0.8511],
        [-0.8046],
        [-0.4976],
        [-0.8442]], device='cuda:0'), 
// ...中间有98个tensor，每个20个数据
 tensor([[-0.2900],
        [-0.6720],
        [-0.7235],
        [-0.8511],
        [-0.8046],
        [-0.4976],
        [-0.8442],
        [-0.6934],
        [-0.8517],
        [-0.7266],
        [-0.5894],
        [ 1.0000],
        [-0.4753],
        [-0.4870],
        [-0.4651],
        [-0.6508],
        [-0.4771],
        [-0.4479],
        [-0.5342],
        [-0.6291]], device='cuda:0')
)
 
// 将100个(20,1)按列竖着拼接成(100,20,1)，得到模型的输入数据
// in
 tensor([[[-0.5161],
         [-0.5684],
         [-0.5169],
         ...,
         [-0.8046],
         [-0.4976],
         [-0.8442]],

        [[-0.5684],
         [-0.5169],
         [-0.6286],
         ...,
         [-0.4976],
         [-0.8442],
         [-0.6934]],

        [[-0.5169],
         [-0.6286],
         [-0.3985],
         ...,
         [-0.8442],
         [-0.6934],
         [-0.8517]],

        ...,

        [[-0.5884],
         [-0.7686],
         [-0.4507],
         ...,
         [-0.7260],
         [-0.6980],
         [-0.5960]],

        [[-0.7686],
         [-0.4507],
         [-0.5368],
         ...,
         [-0.6980],
         [-0.5960],
         [-0.5318]],

        [[-0.4507],
         [-0.5368],
         [-0.4574],
         ...,
         [-0.5960],
         [-0.5318],
         [-0.4263]]], device='cuda:0')
```

对应的模型输出out也一样

```json
// out
tensor([[[-0.5684],
         [-0.5169],
         [-0.6286],
         ...,
         [-0.4976],
         [-0.8442],
         [-0.6934]],

        [[-0.5169],
         [-0.6286],
         [-0.3985],
         ...,
         [-0.8442],
         [-0.6934],
         [-0.8517]],

        [[-0.6286],
         [-0.3985],
         [-0.5671],
         ...,
         [-0.6934],
         [-0.8517],
         [-0.7266]],

        ...,

        [[-0.7686],
         [-0.4507],
         [-0.5368],
         ...,
         [-0.6980],
         [-0.5960],
         [-0.5318]],

        [[-0.4507],
         [-0.5368],
         [-0.4574],
         ...,
         [-0.5960],
         [-0.5318],
         [-0.4263]],

        [[-0.5368],
         [-0.4574],
         [-0.6373],
         ...,
         [-0.5318],
         [-0.4263],
         [-0.6205]]], device='cuda:0')
```

**具体数据dem**

- 一开始的20个数据就是20个依次滑动的窗口

```json
[1,2,3...100]
[2,3,4...101]
[3,4,5...102]
...
[20,21,22...121]
```

- 因此按列分割，拼接到一起后为

```json
[
    [
        [1]
        [2]
        [3]
        ...
        [20]
    ]
    ,
    [
        [2]
        [3]
        [4]
        ...
        [21]
    ]
    ,
    ...
    [
        [100]
        [101]
        [102]
        ...
        [121]
    ]
]
```

## 掩码

根据输入数据的规模**(100,20,1)**生成一个**上三角矩阵mask**，规模是**(100,100)**

- torch.ones(n, m)：返回一个n*m的tensor
- torch.triu(input, diagonal=0, out=None)→Tensor
  - input即**(sz,sz)**的tensor
  - diagonal为空**保留输入矩阵主对角线与主对角线以上的元素**，其他元素置0
  - 最后转置
- 在掩蔽的位置填充float('-inf')，**正常位置填充float(0.0)**
  - 负无穷float('-inf')经过softmax后，权重就会接近0

```python
def _generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        # masked_fill(mask, value) → tensor，在mask值为1的位置处用value填充,mask的元素个数需要和tensor相同,但尺寸可以不同
        mask = mask.float().masked_fill(mask == 0, float(
            '-inf')).masked_fill(mask == 1, float(0.0))
        return mask
```

```bash
# torch.triu(torch.ones(sz, sz))生成的上三角tensor
tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [0., 1., 1.,  ..., 1., 1., 1.],
        [0., 0., 1.,  ..., 1., 1., 1.],
        ...,
        [0., 0., 0.,  ..., 1., 1., 1.],
        [0., 0., 0.,  ..., 0., 1., 1.],
        [0., 0., 0.,  ..., 0., 0., 1.]])

# 虽然生成的上三角矩阵，但是用1来判断后转置了，所以还是遮蔽上半部分
tensor([[ True, False, False,  ..., False, False, False],
        [ True,  True, False,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        ...,
        [ True,  True,  True,  ...,  True, False, False],
        [ True,  True,  True,  ...,  True,  True, False],
        [ True,  True,  True,  ...,  True,  True,  True]])

# 最后的mask矩阵
tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],
        [0., 0., -inf,  ..., -inf, -inf, -inf],
        [0., 0., 0.,  ..., -inf, -inf, -inf],
        ...,
        [0., 0., 0.,  ..., 0., -inf, -inf],
        [0., 0., 0.,  ..., 0., 0., -inf],
        [0., 0., 0.,  ..., 0., 0., 0.]])
```

# forward

输入**(100,20,1)**

```bash
tensor([[[-0.5161],
         [-0.5684],
         [-0.5169],
         ...,
         [-0.8046],
         [-0.4976],
         [-0.8442]],
        ...,
        [[-0.7686],
         [-0.4507],
         [-0.5368],
         ...,
         [-0.6980],
         [-0.5960],
         [-0.5318]],

        [[-0.4507],
         [-0.5368],
         [-0.4574],
         ...,
         [-0.5960],
         [-0.5318],
         [-0.4263]]], device='cuda:0')
```



## 位置编码

**(100,20,1)先输入到位置编码的forward中**

- **in = (S,N,E)，即(sequence_len,batch_size,d_model)**
  - S是source sequence length
  - N是batch size
  - E是feature number


- pe = (5000,1,250)，5000是序列最大长度（max_sequence_len），250是d_model，单个数据表示成d_model长度的向量
- 所以**取x.size(0)=100对pe进行划分，得到(100,1,250)的位置数据**，加到输入in上

> 所以pe要改变形状
>
> x.size(0) = 100，即input_window

```python
return x + self.pe[:x.size(0), :]
```

```bash
# self.pe[:x.size(0), :]，(100,1,250)
tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,
           0.0000e+00,  1.0000e+00]],

        [[ 8.4147e-01,  5.4030e-01,  8.0100e-01,  ...,  1.0000e+00,
           1.0765e-04,  1.0000e+00]],

        [[ 9.0930e-01, -4.1615e-01,  9.5906e-01,  ...,  1.0000e+00,
           2.1529e-04,  1.0000e+00]],

        ...,

        [[ 3.7961e-01, -9.2515e-01,  8.3955e-01,  ...,  9.9994e-01,
           1.0442e-02,  9.9995e-01]],

        [[-5.7338e-01, -8.1929e-01,  6.7432e-02,  ...,  9.9994e-01,
           1.0549e-02,  9.9994e-01]],

        [[-9.9921e-01,  3.9821e-02, -7.5881e-01,  ...,  9.9993e-01,
           1.0657e-02,  9.9994e-01]]], device='cuda:0')
```

### 广播

因为形状不一样，会触发**广播**特性

- pe取100个数据，每个数据1个窗口，每个窗口250个值
- x取100行数据，每个数据20个窗口，每个窗口只有1个值

所以会自动对x和pe进行扩展，**将pe和x变成(100,20,250)**

- pe：100个数据，**每个数据复制成20个窗口**，每个窗口250个值
- x：100个数据，每个数据20个窗口，**每个窗口复制成250个值**

最后相加，**输入x变成一个(100,20,250)的数据**

> 100行必须是一样的，即x.size(0)
>
> 复制值，不会变

```json
// 添加位置编码后的输入in，(100,20,250)
tensor([[[-0.5161,  0.4839, -0.5161,  ...,  0.4839, -0.5161,  0.4839],
         [-0.5684,  0.4316, -0.5684,  ...,  0.4316, -0.5684,  0.4316],
         [-0.5169,  0.4831, -0.5169,  ...,  0.4831, -0.5169,  0.4831],
         ...,
         [-0.8046,  0.1954, -0.8046,  ...,  0.1954, -0.8046,  0.1954],
         [-0.4976,  0.5024, -0.4976,  ...,  0.5024, -0.4976,  0.5024],
         [-0.8442,  0.1558, -0.8442,  ...,  0.1558, -0.8442,  0.1558]],

        [[ 0.2731, -0.0281,  0.2326,  ...,  0.4316, -0.5682,  0.4316],
         [ 0.3246,  0.0234,  0.2841,  ...,  0.4831, -0.5168,  0.4831],
         [ 0.2128, -0.0883,  0.1724,  ...,  0.3714, -0.6285,  0.3714],
         ...,
         [ 0.3439,  0.0427,  0.3034,  ...,  0.5024, -0.4975,  0.5024],
         [-0.0027, -0.3039, -0.0432,  ...,  0.1558, -0.8441,  0.1558],
         [ 0.1480, -0.1531,  0.1076,  ...,  0.3066, -0.6933,  0.3066]],

        [[ 0.3924, -0.9330,  0.4422,  ...,  0.4831, -0.5167,  0.4831],
         [ 0.2807, -1.0448,  0.3304,  ...,  0.3714, -0.6284,  0.3714],
         [ 0.5108, -0.8146,  0.5606,  ...,  0.6015, -0.3983,  0.6015],
         ...,
         [ 0.0651, -1.2603,  0.1149,  ...,  0.1558, -0.8440,  0.1558],
         [ 0.2159, -1.1096,  0.2656,  ...,  0.3066, -0.6932,  0.3066],
         [ 0.0576, -1.2679,  0.1073,  ...,  0.1483, -0.8515,  0.1483]],

        ...,

        [[-0.2088, -1.5136,  0.2511,  ...,  0.4115, -0.5780,  0.4115],
         [-0.3890, -1.6938,  0.0709,  ...,  0.2313, -0.7582,  0.2313],
         [-0.0711, -1.3758,  0.3888,  ...,  0.5492, -0.4403,  0.5493],
         ...,
         [-0.3464, -1.6511,  0.1136,  ...,  0.2740, -0.7155,  0.2740],
         [-0.3184, -1.6232,  0.1415,  ...,  0.3019, -0.6876,  0.3019],
         [-0.2164, -1.5212,  0.2435,  ...,  0.4039, -0.5856,  0.4039]],

        [[-1.3420, -1.5879, -0.7012,  ...,  0.2313, -0.7581,  0.2313],
         [-1.0241, -1.2700, -0.3833,  ...,  0.5492, -0.4401,  0.5492],
         [-1.1102, -1.3561, -0.4694,  ...,  0.4631, -0.5263,  0.4631],
         ...,
         [-1.2714, -1.5173, -0.6306,  ...,  0.3019, -0.6875,  0.3019],
         [-1.1694, -1.4153, -0.5286,  ...,  0.4039, -0.5855,  0.4039],
         [-1.1052, -1.3511, -0.4644,  ...,  0.4681, -0.5213,  0.4681]],

        [[-1.4499, -0.4109, -1.2095,  ...,  0.5492, -0.4400,  0.5492],
         [-1.5360, -0.4970, -1.2956,  ...,  0.4631, -0.5262,  0.4631],
         [-1.4566, -0.4176, -1.2162,  ...,  0.5425, -0.4467,  0.5425],
         ...,
         [-1.5952, -0.5562, -1.3548,  ...,  0.4039, -0.5854,  0.4039],
         [-1.5311, -0.4920, -1.2907,  ...,  0.4681, -0.5212,  0.4681],
         [-1.4255, -0.3864, -1.1851,  ...,  0.5737, -0.4156,  0.5737]]],
       device='cuda:0')
```

## 多维

> 如果是多维，那输入应该就是(100,20,250)
>
> 就只用广播pe(100,1,250)的中间维度

## Encoder

进入encoder中

- **输入x**

  - 归一化：**x1 = norm(x)**

  - 进入multi-head self-attention：**x2 = self_attention(x1)**

  - 残差加成：**x3 = x + x2**

  - 归一化：**x4 = norm(x3)**

  - 进入前馈网络：**x5 = feed_forward(x4)**

  - 残差加成：**x6 = x3 + x5**


- **输出x6**

Encoder的**输出结果依旧是一个(100,20,250)的数据**

```json
tensor([[[-0.9860,  1.4549, -0.5863,  ...,  0.7547, -0.8560,  0.6348],
         [-1.1977,  1.6100, -0.6696,  ...,  0.8074, -0.6878,  0.5937],
         [-1.4577,  1.4083, -0.6364,  ...,  1.1334, -0.5294,  0.4869],
         ...,
         [-1.1347,  0.4049,  0.0073,  ...,  0.4853, -0.6947,  0.4562],
         [-1.3624,  1.1277, -0.8822,  ...,  0.8259, -1.0095,  0.5552],
         [-1.2334,  1.6753, -0.7525,  ...,  0.9711, -0.3761,  0.3268]],

        [[ 0.1826,  0.6501,  0.7719,  ...,  1.0858, -1.1150,  0.4773],
         [ 0.1684,  1.0679,  0.7394,  ...,  1.0630, -1.0512,  0.5670],
         [ 0.1491,  1.1002,  0.7501,  ...,  0.9424, -0.7955,  1.0871],
         ...,
         [ 0.4979,  0.6830,  1.1991,  ...,  1.0741, -1.0546,  0.1389],
         [ 0.0785,  0.7723, -0.1703,  ...,  1.1660, -0.6914,  0.5123],
         [ 0.2605,  0.6612,  0.7932,  ...,  0.8354, -0.9284,  0.2744]],

        [[ 0.5926, -1.5333,  1.0270,  ...,  0.5408, -0.6057,  1.0573],
         [ 0.4862, -1.2018,  0.2325,  ...,  0.7061, -0.7060,  0.7700],
         [ 0.2921, -2.2663,  1.1903,  ...,  0.7084, -0.7757,  0.9924],
         ...,
         [ 0.3766, -1.2757,  0.7826,  ...,  0.6579, -0.9040,  0.5178],
         [ 0.5746, -1.2710,  1.1072,  ...,  0.5912, -0.8151,  0.4637],
         [ 0.3280, -2.2059,  0.8934,  ...,  0.6481, -0.3552,  0.4790]],

        ...,

        [[-0.1850, -1.0882,  1.0251,  ...,  1.3383, -0.2696,  0.9187],
         [-0.0775, -0.9473,  1.0650,  ...,  1.1225,  0.2002,  0.9088],
         [-0.2040, -1.1715,  1.4754,  ...,  0.7107,  0.2010,  0.8810],
         ...,
         [ 0.1238, -1.2066,  0.1894,  ...,  1.1650, -0.2350,  1.1211],
         [-0.1642, -1.2181,  0.9809,  ...,  1.0987,  0.2132,  1.0066],
         [-0.1010, -1.2287,  0.9529,  ...,  1.1500,  0.1023,  1.0381]],

        [[-1.5313, -0.9177, -0.1623,  ...,  1.0885,  0.1501,  1.1088],
         [-1.5778, -1.0709, -0.1237,  ...,  1.2339,  0.1552,  0.9182],
         [-1.6446, -0.9662, -0.1449,  ...,  0.6577,  0.2154,  1.0677],
         ...,
         [-1.5515, -0.8109, -0.2435,  ...,  1.0433,  0.0509,  1.2033],
         [-1.6830, -1.0061,  0.3854,  ...,  0.6131,  0.0752,  1.1032],
         [-1.4651, -1.1838, -0.2480,  ...,  1.1751,  0.1961,  1.0650]],

        [[-2.1493,  0.2221, -1.3728,  ...,  1.1773,  0.1187,  0.9726],
         [-1.8473,  0.2098, -1.4540,  ...,  1.3061, -0.0753,  1.2810],
         [-1.9592,  0.2191, -1.3524,  ...,  1.3175,  0.0681,  0.9768],
         ...,
         [-1.8302,  0.1964, -0.8726,  ...,  1.2006, -0.0210,  1.0308],
         [-2.2398,  0.2965, -1.3614,  ...,  0.7164,  0.0444,  1.0879],
         [-2.1661,  0.3775, -1.5071,  ...,  1.3114, -0.0711,  0.8497]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward>)
```

## Decoder

最后进入decoder中

- decoder的liner将250维的数据得到1维的数据，**最终输出一个(100,20,1)的数据**，这就是本次训练的结果

> 对应之前的out数据， 相同维度然后算loss

```json
tensor([[[1.7166],
         [1.5955],
         [1.3612],
         ...,
         [1.2925],
         [0.9609],
         [1.1833]],

        [[1.4403],
         [1.3335],
         [1.7375],
         ...,
         [1.8072],
         [1.2017],
         [1.4680]],

        [[1.6215],
         [1.5733],
         [1.0512],
         ...,
         [1.6895],
         [1.3844],
         [1.1320]],

        ...,

        [[0.3132],
         [0.4665],
         [0.4616],
         ...,
         [0.4908],
         [0.3903],
         [0.5183]],

        [[0.7226],
         [0.4793],
         [0.5261],
         ...,
         [0.5081],
         [0.6604],
         [0.7706]],

        [[0.6698],
         [0.7162],
         [0.8714],
         ...,
         [0.9011],
         [0.5640],
         [0.9288]]], device='cuda:0', grad_fn=<AddBackward0>)
```

## loss

计算本次的loss

- decoder的数据和out数据**(100,20,1)**做计算，得到loss
- 计算MSELoss = **(x-y)^2**

> 例如：第一次计算loss = 1.5147678852081299

```json
// out
tensor([[[-0.5684],
         [-0.5169],
         [-0.6286],
         ...,
         [-0.4976],
         [-0.8442],
         [-0.6934]],
        ...,
        [[-0.5368],
         [-0.4574],
         [-0.6373],
         ...,
         [-0.5318],
         [-0.4263],
         [-0.6205]]], device='cuda:0')
```

## log

一个epoch会拿一整个完整的数据集分batch_size大小进行训练，所以训练次数为：**数据集长度 / batch_size = 1899 / 20 = 94**，即一个Epoch有94个batch（94轮）

- 每5轮打印一次日志，并计算这5次的平均loss，即间隔为：`log_interval = int(len(train_data) / batch_size / 5) = 18`

> 每5轮打印一次日志，所以是18的倍数增加

```json
log_interval = int(len(train_data) / batch_size / 5)

print('|epoch{:3d}|{:5d}/{:5d} batches | '
      'lr {:02.6f} | {:5.2f} ms | '
      'loss {:5.5f} | ppl {:8.2f}'.format(epoch, batch, len(train_data), batch_size, scheduler.get_lr()[0], elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))
```

**日志结**

```bash
| epoch   1 |    18/   94 batches | lr 0.005000 | 3061.06 ms | loss 0.69152 | ppl     2.00
| epoch   1 |    36/   94 batches | lr 0.005000 |  7.65 ms | loss 0.16171 | ppl     1.18
| epoch   1 |    54/   94 batches | lr 0.005000 |  7.11 ms | loss 0.08397 | ppl     1.09
| epoch   1 |    72/   94 batches | lr 0.005000 |  6.20 ms | loss 0.05685 | ppl     1.06
| epoch   1 |    90/   94 batches | lr 0.005000 |  5.93 ms | loss 0.05175 | ppl     1.05
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 60.78s | valid loss 0.02267 | valid ppl     1.02
-----------------------------------------------------------------------------------------
| epoch   2 |    18/   94 batches | lr 0.004513 |  6.20 ms | loss 0.04733 | ppl     1.05
| epoch   2 |    36/   94 batches | lr 0.004513 |  5.93 ms | loss 0.03837 | ppl     1.04
| epoch   2 |    54/   94 batches | lr 0.004513 |  6.21 ms | loss 0.03602 | ppl     1.04
| epoch   2 |    72/   94 batches | lr 0.004513 |  6.03 ms | loss 0.03306 | ppl     1.03
| epoch   2 |    90/   94 batches | lr 0.004513 |  5.92 ms | loss 0.03799 | ppl     1.04
-----------------------------------------------------------------------------------------
| end of epoch   2 | time:  0.58s | valid loss 0.01588 | valid ppl     1.02
-----------------------------------------------------------------------------------------
| epoch   3 |    18/   94 batches | lr 0.004287 |  6.12 ms | loss 0.03944 | ppl     1.04
| epoch   3 |    36/   94 batches | lr 0.004287 |  5.97 ms | loss 0.03322 | ppl     1.03
| epoch   3 |    54/   94 batches | lr 0.004287 |  5.96 ms | loss 0.03240 | ppl     1.03
| epoch   3 |    72/   94 batches | lr 0.004287 |  5.93 ms | loss 0.03078 | ppl     1.03
| epoch   3 |    90/   94 batches | lr 0.004287 |  5.99 ms | loss 0.03557 | ppl     1.04
-----------------------------------------------------------------------------------------
| end of epoch   3 | time:  0.57s | valid loss 0.01584 | valid ppl     1.02
```

# 可视化

训练完成后，需要可视化loss，每10个epoch一次

- 使用划分的验证集来计算，一共是**203 - 100 - 1 = 102**个数据**(102,2,100)**

```bash
tensor([
		[
            [-0.5739, -0.4021, -0.5595,  ..., -0.5235, -0.5567, -0.6140],
             [-0.4021, -0.5595, -0.6043,  ..., -0.5567, -0.6140, -0.5029]
         ],
        ...,
        [
            [-0.6021, -0.5862, -0.5648,  ..., -0.6812, -0.5717, -0.8837],
             [-0.5862, -0.5648, -0.2523,  ..., -0.5717, -0.8837, -0.6390]
         ]
        ],device='cuda:0')
```

使用batch_size = 1来划分输入和输出，所以数据为**(100,1,1)**

```bash
# in
tensor([[[-0.5739]],

    	...
    	
        [[-0.5235]],

        [[-0.6140]]], device='cuda:0')
```

计算整个batch（即验证集的一个epoch）的total_loss

```python
total_loss += criterion(output, target).item()
```

拼接验证集的网络输出和labe

- 最后两者都是**(101)**，即101个batch的数据的输出和label

```python
# 初始化
test_result = torch.Tensor(0)
truth = torch.Tensor(0)

# 累积拼接output[-1]
test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0)
# 累积拼接target[-1]
truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)
```

```bash
# output
tensor([[[-0.5109]],
		
		...

        [[-0.5407]],

        [[-0.5148]]], device='cuda:0')

# target
tensor([[[-0.6021]],

       ...

        [[-0.5717]],

        [[-0.8837]]], device='cuda:0')
```

