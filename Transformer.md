# Self-Attention

## 矩阵计算

$XX^T$，其中
$$
X = (x_1^t,x_2^t,x_3^t)\\
假设：x_1 = [1,2,1,2,1]^T,x_2 = [1,2,1,1,1]^T,x_3 = [2,1,1,2,1]^T\\
只计算x_1:x_1X^T = x_1(x_1^t,x_2^t,x_3^t)
\left[\begin{matrix}1&2&1&2&1\end{matrix}\right]
\left[\begin{matrix}1&1&2\\2&2&1\\1&1&1\\2&1&2\\1&1&1\end{matrix}\right]
=[11,9,10]
$$
矩阵计算可以看出行向量和列向量的**内积**

**内积的几何意义**：是一个向量在另一个向量上的投影

- 投影的值大，说明两个向量的相关度高
- 如果两个向量夹角是九十度，内积为0，即这两个向量线性无关，完全没有相关性

> 所以[11,9,10]表示x1和x1,x2,x3的相关性

## Softmax

$Softmax(XX^T)$

**归一化**

- 让结果的这些概率的和为1

每一个向量和其他向量的结果进过归一化处理后得到的各个概率就是应该分配的注意力

**计算**

- **为了避免求$e^x$出现溢出的情况，一般需要减去最大值**

> softmax([11,9,10] = [0.67,0.09,0.24])

## 加权求和

$Softmax(XX^T)X$

矩阵乘法归一化的结果再和矩阵相乘，此时的结果的每一个行向量都和X的维度相同

- 每一个行向量的分量（每一个维度的数值）都是由三个向量**在这一维度的数值加权求和得到**
- 权重就是softmax后得到的相关性
- **这个新的行向量就是x1向量经过注意力机制加权求和之后得到的表示**

$$
Softmax(XX^T)X\\
只计算x_1:Softmax(x_1X^T)X = 
\left[\begin{matrix}0.67&0.09&0.24\end{matrix}\right]
\left[\begin{matrix}1&2&1&2&1\\1&2&1&1&1\\1&1&1&2&1\end{matrix}\right]
=\left[\begin{matrix}1&1.76&1&1.91&1\end{matrix}\right]
$$



## QKV模型

QKV矩阵本质上都是矩阵X的一个线性变换
$$
Q = W^qX\\
K = W^kX\\
V = W^vX
$$
不直接使用矩阵X来进行计算而对它进行线性变换是为了提高模型的拟合能力，因为W矩阵是可以通过训练优化的