传统seq2seq模型最大的问题在于

- **将Encoder端的所有信息压缩到一个固定长度的向量中**，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词（token）的隐藏状态。在输入序列比较长的时候，这样做会损失Encoder端的很多信息
- 而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息
- 并且模型计算不可并行，会耗费大量时间

Transformer

- **基于encoder-decoder架构**，抛弃了传统的RNN、CNN模型，**仅由Attention机制实现**，解决了输入输出的长期依赖问题

- **encoder端是并行计算的，训练时间大大缩短，减少了计算资源的消耗**
- self-attention模块**让源序列和目标序列首先自关联起来**，这样源序列和目标序列**自身的embedding表示所蕴含的信息更加丰富**
- 后续的FFN层也增强了模型的表达能力
- Muti-Head Attention模块使得Encoder端拥有并行计算的能力
  - Decoder端仍旧需要串行计算

# Self-Attention

## 矩阵计算

$XX^T$，其中
$$
X = (x_1^t,x_2^t,x_3^t)\\
假设：x_1 = [1,2,1,2,1]^T,x_2 = [1,2,1,1,1]^T,x_3 = [2,1,1,2,1]^T\\
只计算x_1:x_1X^T = x_1(x_1^t,x_2^t,x_3^t)
\left[\begin{matrix}1&2&1&2&1\end{matrix}\right]
\left[\begin{matrix}1&1&2\\2&2&1\\1&1&1\\2&1&2\\1&1&1\end{matrix}\right]
=[11,9,10]
$$
矩阵计算可以看出行向量和列向量的**内积**

**内积的几何意义**：是一个向量在另一个向量上的投影

- 投影的值大，说明两个向量的相关度高
- 如果两个向量夹角是九十度，内积为0，即这两个向量线性无关，完全没有相关性

> 所以[11,9,10]表示x1和x1,x2,x3的相关性

## Softmax

**hardmax**是求数组所有元素中值最大的元素，只选出其中一个最大的值。但是往往在实际中这种方式是不合情理的

Softmax的含义就在于不再唯一的确定某一个最大值，而是**为每个输出分类的结果都赋予一个概率值**，**表示属于每个类别的可能性**

$Softmax(XX^T)$

**归一化**

- 将多分类的结果输出值转换为**范围在[0, 1]和为1的概率分布**

- 这其中每一个向量和其他向量的结果进过归一化处理后得到的各个概率就是应该分配的注意力

**指数函数**

- 指数函数曲线呈现递增趋势，斜率逐渐增大。所以x的很小的变化，可以导致y的很大的变化，所以指数函数能够将**输出的数值拉开距离**
- 在使用反向传播求解梯度更新参数的过程中，指数函数在求导的时候比较方便

**计算**

- 指数函数值可能会非常大
- 为了避免求$e^x$出现溢出的情况，一般**需要减去最大值**，即$e^{x-max}$

> softmax([11,9,10] = [0.67,0.09,0.24])

**损失函数**

- 使用Softmax函数作为输出节点的**激活函数**的时候，一般使用交叉熵作为损失函数
- 由于Softmax函数的数值计算过程中，很容易因为输出节点的输出值比较大而发生数值溢出的现象，在计算交叉熵的时候也可能会出现数值溢出的问题
- 为了数值计算的稳定性，TensorFlow提供了一个统一的接口，将Softmax与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常

## 加权求和

$Softmax(XX^T)X$

矩阵乘法归一化的结果再和矩阵相乘，此时的结果的每一个行向量都和X的维度相同

- 每一个行向量的分量（每一个维度的数值）都是由三个向量**在这一维度的数值加权求和得到**
- 权重就是softmax后得到的相关性
  - 即Query与Key作用得到attention的权值

- **这个新的行向量就是x1向量经过注意力机制加权求和之后得到的表示**
  - 即权值作用在Value上得到attention值


$$
Softmax(XX^T)X\\
只计算x_1:Softmax(x_1X^T)X = 
\left[\begin{matrix}0.67&0.09&0.24\end{matrix}\right]
\left[\begin{matrix}1&2&1&2&1\\1&2&1&1&1\\1&1&1&2&1\end{matrix}\right]
=\left[\begin{matrix}1&1.76&1&1.91&1\end{matrix}\right]
$$

> 通过query和key的相似性程度来确定value的权重分布的方法被称为scaled dot-product attention
>

## QKV模型

Q，K，V矩阵本质上都是矩阵X的**一个线性变换**
$$
Q = W^qX\\
K = W^kX\\
V = W^vX
$$
不直接使用矩阵X来进行计算而对它进行线性变换是为了提高模型的拟合能力，因为**W矩阵是可以通过训练优化的**

## 缩放因子

$\sqrt{d_k}$

- 假设Q，K中元素的均值为0，方差为1。则A = QTK的元素均值为0，方差为d
- 当d变得很大时，A的元素的方差也会很大，那么softmax(A)的分布就会趋于陡峭
  - 分布的方差大，分布集中在绝对值大的区域
- A中每个元素缩放后，方差又变为1，则softmax(A)的分布与d无关了

## 位置编码

- **对self attention来说，它对每一个input vector都做attention，所以没有考虑到input sequence的顺序**，它缺少了原本的位置信息
  - 没有说像RNN那样**后面的输入考虑了前面输入的信息**
  - 也**没有考虑输入向量之间的距离远近**
- 对比来说，LSTM是对于文本顺序信息的解释是输出词向量的先后顺序，而self attention的计算对sequence的顺序这一部分则完全没有提及，打乱sequence的顺序，得到的结果仍然是相同的

Positional Encoding是transformer的特有机制，弥补了Attention机制无法捕捉sequence中token位置信息的缺点

- 为每个位置的输入都设定一个独立的**位置向量**ei
- 将位置向量加到输入向量上

# Muti-Head attention

Muti-Head attention的每个头**关注序列的不同位置**，增强了Attention机制关注**序列内部向量之间**作用的表达能力
$$
Q_i = W_i^qX\\
K_i = W_i^kX\\
V_i = W_i^vX
$$
通过权重矩阵$W_i^{q,k,v}$将Q，K，V分割，因为$W_i^{q,k,v}$各不相同，所以计算的Qi，Ki，Vi也都不相同，即 ：**每个头关注的重点不一样**

> Attention是将query和key映射到**同一高维空间中去计算相似度**，而对应的multi-head attention把query和key映射到高维空间α的不同子空间（α1，α2，α3...）取计算相似度

# Feed Forward NN

前馈神经网络/全连接神经网络

- FNN为encoder引入**非线性变换**（ReLU激活函数），变化了attention的输出空间，增强了模型的拟合能力