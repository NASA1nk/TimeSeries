# Self-Attention

## 矩阵计算

$XX^T$，其中
$$
X = (x_1^t,x_2^t,x_3^t)\\
假设：x_1 = [1,2,1,2,1]^T,x_2 = [1,2,1,1,1]^T,x_3 = [2,1,1,2,1]^T\\
只计算x_1:x_1X^T = x_1(x_1^t,x_2^t,x_3^t)
\left[\begin{matrix}1&2&1&2&1\end{matrix}\right]
\left[\begin{matrix}1&1&2\\2&2&1\\1&1&1\\2&1&2\\1&1&1\end{matrix}\right]
=[11,9,10]
$$
矩阵计算可以看出行向量和列向量的**内积**

**内积的几何意义**：是一个向量在另一个向量上的投影

- 投影的值大，说明两个向量的相关度高
- 如果两个向量夹角是九十度，内积为0，即这两个向量线性无关，完全没有相关性

> 所以[11,9,10]表示x1和x1,x2,x3的相关性

## Softmax

$Softmax(XX^T)$

**归一化**

- 让结果的这些概率的和为1

每一个向量和其他向量的结果进过归一化处理后得到的各个概率就是应该分配的注意力

**计算**

- **为了避免求$e^x$出现溢出的情况，一般需要减去最大值**

> softmax([11,9,10] = [0.67,0.09,0.24])

## 加权求和

$Softmax(XX^T)X$

矩阵乘法归一化的结果再和矩阵相乘，此时的结果的每一个行向量都和X的维度相同

每一个行向量的分量（每一个维度的数值）都是由三个向量在这一维度的数值加权求和得到
$$
X = (x_1^t,x_2^t,x_3^t)\\
假设：x_1 = [1,2,1,2,1]^T,x_2 = [1,2,1,1,1]^T,x_3 = [2,1,1,2,1]^T\\
只计算x_1:x_1X^T = x_1(x_1^t,x_2^t,x_3^t)
\left[\begin{matrix}1&2&1&2&1\end{matrix}\right]
\left[\begin{matrix}1&1&2\\2&2&1\\1&1&1\\2&1&2\\1&1&1\end{matrix}\right]
=[11,9,10]
$$
